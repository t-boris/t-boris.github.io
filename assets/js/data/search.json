[ { "title": "ELK on EKS cluster", "url": "/posts/elk-on-eks/", "categories": "devops", "tags": "k8s, eks, elk", "date": "2023-01-24 00:00:00 +0000", "snippet": "IntroductionThis post provides instructions for setting up EKS kubernetes cluster with Elastic stack.One of my projects required deployment to kubernetes cluster and since this was a new project I ...", "content": "IntroductionThis post provides instructions for setting up EKS kubernetes cluster with Elastic stack.One of my projects required deployment to kubernetes cluster and since this was a new project I decided to do it completely in the cloud. So my obvious choice was EKS from AWS as a main provider of cloud technology. My next task was to set up cross-cutting concerns for the application. First default requirement was setting up logging system. I decided to use Elastic search with Kibana for this purpose.The following post contains steps that I did to generate such environmentEKSI decided to use CDK in order to manipulate my AWS environment. Prerequisites for the CDK are: AWS Account AWS CLI CDK CLIIn order to generate CDK project type (I decided to use go version of the tool)cdk init app --language goModify app.gopackage main import ( \"github.com/aws/aws-cdk-go/awscdk/v2\" \"github.com/aws/aws-cdk-go/awscdk/v2/awsautoscaling\" \"github.com/aws/aws-cdk-go/awscdk/v2/awsec2\" \"github.com/aws/aws-cdk-go/awscdk/v2/awseks\" \"github.com/aws/aws-cdk-go/awscdk/v2/awsiam\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/constructs-go/constructs/v10\" \"github.com/aws/jsii-runtime-go\") type AwsStackProps struct { awscdk.StackProps } func NewAwsStack(scope constructs.Construct, id string, props *AwsStackProps) awscdk.Stack { \tvar sprops awscdk.StackProps \tif props != nil { sprops = props.StackProps \t} \tstack := awscdk.NewStack(scope, &amp;id, &amp;sprops) // Create VPC \tvpc := awsec2.NewVpc(stack, aws.String(\"my-vpc\"), &amp;awsec2.VpcProps{}) // IAM role for our EC2 worker nodes \tworkerRole := awsiam.NewRole(stack, jsii.String(\"EKSWorkerRole\"), &amp;awsiam.RoleProps{ AssumedBy: awsiam.NewServicePrincipal(jsii.String(\"ec2.amazonaws.com\"), nil), }) // Create Kubernetes cluster \tcluster := awseks.NewCluster(stack, jsii.String(\"my-cluster\"), &amp;awseks.ClusterProps{ ClusterName: jsii.String(\"my-cluster\"), Vpc: vpc, Version: awseks.KubernetesVersion_V1_24(), DefaultCapacityInstance: awsec2.InstanceType_Of(awsec2.InstanceClass_T3, awsec2.InstanceSize_MEDIUM), }) \tonDemandASG := awsautoscaling.NewAutoScalingGroup(stack, jsii.String(\"OnDemandASG\"), &amp;awsautoscaling.AutoScalingGroupProps{ Vpc: vpc, Role: workerRole, MinCapacity: jsii.Number(1), MaxCapacity: jsii.Number(10), InstanceType: awsec2.NewInstanceType(jsii.String(\"t3.medium\")), MachineImage: awseks.NewEksOptimizedImage(&amp;awseks.EksOptimizedImageProps{ KubernetesVersion: jsii.String(\"1.24\"), NodeType: awseks.NodeType_STANDARD, }), }) \tcluster.ConnectAutoScalingGroupCapacity(onDemandASG, &amp;awseks.AutoScalingGroupOptions{})\t\treturn stack }...I configured the cluster to use t3.medium boxes for control plane and for worker nodes.DeploymentPrerequisites for this part is: eksctl helm version 3+Deploy EKS clustercdk deployDeployment of the cluster could take 15 minutes. After that you should register context in kubernetes aws eks update-kubeconfig --region us-east-1 --name my-cluster --role-arn &lt;role arn&gt;Example:aws eks update-kubeconfig --name my-cluster --region us-east-1 --role-arn arn:aws:iam::11111111111:role/AwsStack-myclusterMastersRoleA2A674AE-1K40LT54NLBLLCreate ODIC provider for the clustereksctl utils associate-iam-oidc-provider --cluster=my-cluster --region us-east-1 --approveAdd Amazon EBS CSI driverElasticsearch uses local storage and Kubernetes cluster need a driver to connect to EBS. Basically you need to install addon - EBS CSI drivereksctl create iamserviceaccount \\ \t--name ebs-csi-controller-sa \\ \t--namespace kube-system \\ \t--cluster my-cluster \\ \t--attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\ \t--approve \\ \t--role-only \\ \t--role-name AmazonEKS_EBS_CSI_DriverRole eksctl create addon --name aws-ebs-csi-driver --cluster my-cluster --service-account-role-arn arn:aws:iam::111111111111:role/AmazonEKS_EBS_CSI_DriverRole --forceCreate EBS Storage ClassCreate file storage-class.yaml with contentkind: \"StorageClass\" apiVersion: \"storage.k8s.io/v1\" metadata: name: \"standard\" annotations: \t storageclass.kubernetes.io/is-default-class: \"true\"provisioner: \"kubernetes.io/aws-ebs\" parameters: type: \"gp2\" fsType: \"ext4\" Apply the yaml file with kubectlkubectl apply -f storage-class.yamlELKAdd elastic repository and create namespace for the ELKhelm repo add elastic https://helm.elastic.cohelm repo update kubectl create namespace monitoringElasticsearchCreate file values.yaml with content:--- # Permit co-located instances for solitary minikube virtual machines. antiAffinity: \"soft\" # Shrink default JVM heap. esJavaOpts: \"-Xmx512m -Xms512m\" # Allocate smaller chunks of memory per pod. resources: requests: \t cpu: \"100m\" \t memory: \"1Gi\" limits: \t cpu: \"1000m\" \t memory: \"1Gi\" # Request smaller persistent volumes. volumeClaimTemplate: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"standard\" resources: \trequests:\t\tstorage: 10Gi Install elastichelm install elasticsearch elastic/elasticsearch -n monitoring -f values.yaml --set replicas=1Kibanahelm install kibana elastic/kibana -n monitoring --set replicas=1You’ll be able to connect to Kibana with elastic username. In order to get password run the following command:kubectl get secrets --namespace=monitoring elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -dUsageIn order to get access to Kibana run the following commend in a separate window:kubectl port-forward deployment/kibana-kibana 5601 -n monitoringAfter that you can access your cluster through Kibana by URL http://localhost:5601Credentials are elastic and password could be obtained as described aboveHope it helped somebody…" }, { "title": "System Design. Distributed Systems. Concepts", "url": "/posts/system-design-distributed-systems-consepts/", "categories": "system-design", "tags": "cs, system-design, notes", "date": "2023-01-16 00:00:00 +0000", "snippet": "This document contains some notes that I took while studying educative.io courseDistributed SystemsVertical scaling refers to the approach of scaling a system by adding resources (memory, CPU, disk...", "content": "This document contains some notes that I took while studying educative.io courseDistributed SystemsVertical scaling refers to the approach of scaling a system by adding resources (memory, CPU, disk, etc.) to a single node. Meanwhile, horizontal scaling refers to the approach of scaling by adding more nodes to the system.Scalability Scalability lets us store and process datasets much larger than what we could with a single machine.Partitioning Partitioning is the process of splitting a dataset into multiple, smaller datasets, and then assigning the responsibility of storing and processing them to different nodes of a distributed system. This allows us to add more nodes to our system and increase the size of the data it can handle. Vertical Partitioning Horizontal partitioning (Sharding)In a vertically partitioned system, requests that need to combine data from different tables (i.e., join operations) become less efficient. This is because these requests may now have to access data from multiple nodes.In a horizontally partitioned system, we can usually avoid accessing data from multiple nodes because all the data for each row is located in the same node. However, we may still need to access data from multiple nodes for requests that are searching for a range of rows that belong to multiple nodes.Another important implication of horizontal partitioning is the potential for loss of transactional semantics.There are a lot of different algorithms we can use to perform horizontal partitioning.Range partitioningRange partitioning is a technique where we split a dataset into ranges according to the value of a specific attribute. We then store each range in a separate node.Hash partitioningHash partitioning is a technique where we apply a hash function to a specific attribute of each row. This results in a number that determines which partition—and, thus, node—this row belongs to.Consistent hashingConsistent hashing is a partitioning technique that is very similar to hash partitioning, but solves the increased data movement problem caused by hash partitioning. For further discussion on this concept, feel free to read the Dynamo paper. Another widely-used system that uses consistent hashing is Apache Cassandra.AvailabilityAvailability refers to the ability of the system to remain functional despite failures in parts of it.ReplicationReplication is the main technique used in distributed systems to increase availability. It consists of storing the same piece of data in multiple nodes (called replicas) so that if one of them crashes, data is not lost, and requests can be served from the other nodes in the meanwhile.Pessimistic replicationPessimistic replication tries to guarantee from the beginning that all the replicas are identical to each other—as if there was only one copy of the data all along.Optimistic replicationOptimistic replication, or lazy replication, allows the different replicas to diverge. This guarantees that they will converge again if the system does not receive any updates, or enters a quiesced state, for a period of time.Single-master replicationSingle-master replication is a technique where we designate a single node amongst the replicas as the leader, or primary, that receives all the updates. This technique is also known as primary-backup replication.Techniques for propagating updatesSynchronous replicationIn synchronous replication, the node replies to the client to indicate the update is complete—only after receiving acknowledgments from the other replicas that they’ve also performed the update on their local storage. This guarantees that the client is able to view the update in a subsequent read after acknowledging it, no matter which replica the client reads from.Asynchronous replicationIn asynchronous replication, the node replies to the client as soon as it performs the update in its local storage, without waiting for responses from the other replicas.Multi-Master ReplicationIn this technique, all replicas are equal and can accept write requests. They are also responsible for propagating the data modifications to the rest of the group.Conflict resolutionThere are many different ways to resolve conflicts, depending on the guarantees the system wants to provide.An important aspect of different approaches to resolving conflicts is whether they do it eagerly or lazily. In the eagerly case, the conflict is resolved during the write operation. In the lazily case, the write operation proceeds to maintain multiple, alternative versions of the data record that are eventually resolved to a single version later on, i.e., during a subsequent read operation.Quorums in Distributed SystemsLet’s consider an example. In a system of three replicas, we can say that writes need to complete in two nodes (as a quorum of two), while reads need to retrieve data from two nodes. This way, we can be sure that the reads will read the latest value. This is because at least one of the nodes in the read quorum will also be included in the latest write quorum.Safety guarantors (ACID)AtomicityIt is challenging to achieve atomicity in a distributed system because of the possibility of partial failures.Atomicity guarantees that a transaction that comprises multiple operations is treated as a single unit. This means that either all operations of the transaction are executed, or none of them is.ConsistencyIt is challenging to achieve consistency because of the network asynchrony. Network asynchrony occurs when different nodes in a network have different values for the current time. The following illustration shows this.Consistency guarantees that a transaction only transitions the database from one valid state to another valid state, while maintaining any database invariants.IsolationIt is challenging to achieve isolation because of the inherent concurrency of distributed systems.Isolation guarantees that even though transactions might run concurrently and have data dependencies, the result is as if one of them was executed at a time and there was no interference between them.DurabilityDurability guarantees that once a transaction is committed, it remains committed even in the case of failure.In the context of single-node, centralized systems, this usually means that completed transactions and their effects are recorded in non-volatile storage.CAP TheoremAccording to the initial statement of the CAP theorem, it is impossible for a distributed data store to provide more than two of the following properties simultaneously: consistency, availability, and partition tolerance. a distributed system can be either consistent or available in the presence of a network partition. “In the case of a network partition (P), the system has to choose between availability (A) and consistency (C) but else (E), when the system operates normally in the absence of network partitions, the system has to choose between latency (L) and consistency (C).”ProofNow, let’s assume that there is a network failure that results in a network partition between the two nodes of the system at some point. A user of the system performs a write, and then a read—even two different users may perform these operations.In that case, the system has two options: It can fail one of the operations, and break the availability property. It can process both the operations, which will return a stale value from the read and break the consistency property.Consistency ModelThe consistency model defines the set of execution histories that are valid in a system.Strong consistency modelThe consistency model defines the set of execution histories that are valid in a system.Usually, the stronger the consistency model a system satisfies, the easier it is to build an application on top of it. This is because the developer can rely on stricter guarantees.List of consistency models Linearizability operations appear to be instantaneous to the external client. The non-linearizability comes from the use of asynchronous replication. When we use a synchronous replication technique, we make the system linearizable. Sequential Consistency Sequential consistency is a weaker consistency model, where operations are allowed to take effect before their invocation or after their completion. As a result, it provides no real-time guarantees. However, operations from different clients have to be seen in the same order by all other clients, and operations of every single client preserve the order specified by its program (in this global order). Causal Consistency In some cases, we don’t need to preserve the ordering specified by each client’s program—as long as causally related operations are displayed in the right order. This is the causal consistency model, which requires that only operations that are causally related need to be seen in the same order by all the nodes. Eventual Consistency There are still even simpler applications that do not have the notion of a cause-and-effect and require an even simpler consistency model. The eventual consistency model is beneficial here. Isolation Levels and AnomaliesThere is still a need for some formal models that define what is possible and what is not in a system’s behavior. These are called isolation levels Serializability: It essentially states that two transactions, when executed concurrently, should give the same result as though executed sequentially. Repeatable read: It ensures that the data once read by a transaction will not change throughout its course. Snapshot isolation: It guarantees that all reads made in a transaction see a consistent snapshot of the database from the point it started and till the transaction commits successfully if no other transaction has updated the same data since that snapshot. Read committed: It does not allow transactions to read data that has not yet been committed by another transaction. Read uncommitted: It is the lowest isolation level and allows the transaction to read uncommitted data by other transactions.Anomaly: Dirty writeA dirty write occurs when a transaction overwrites a value that was previously written by another transaction that is still in-flight and has not been committed yet.Anomaly: Dirty readA dirty read occurs when a transaction reads a value that has been written by another transaction that has not yet been committed.Anomaly: Fuzzy or non-repeatable readA fuzzy or non-repeatable read occurs when a value is retrieved twice during a transaction (without it being updated in the same transaction), and the value is different.Anomaly: Phantom readA phantom read occurs when a transaction does a predicate-based read, and another transaction writes or removes a data item matched by that predicate while the first transaction is still in flight. If that happens, then the first transaction might be acting again on stale data or inconsistent data.Anomaly: Lost updateA lost update occurs when two transactions read the same value and then try to update it to two different values. The end result is that one of the two updates survives, but the process executing the other update is not informed that its update did not take effect. Thus it is called a lost update.Anomaly: Read skewA read skew occurs when there are integrity constraints between two data items that seem to be violated because a transaction can only see partial results of another transaction.Anomaly: Write skewA write skew occurs when two transactions read the same data, but then modify disjoint sets of data.Models" }, { "title": "Garage door opener with Home Assistant", "url": "/posts/homeassistant-garage-opener/", "categories": "smarthome", "tags": "smarthome, home-assistant", "date": "2023-01-11 00:00:00 +0000", "snippet": "Problem definitionRecently I got a new interest - home automation. This is very broad topic that could require a whole book or a web site. Here I’d like to tell you only the story of automating my ...", "content": "Problem definitionRecently I got a new interest - home automation. This is very broad topic that could require a whole book or a web site. Here I’d like to tell you only the story of automating my garage door.I had the following requirement from this project: Add ability to open my garage with the phone. Don’t like old remotes that I can forget. Ability to perform automations like open garage when my car arrives and automatically close it when it’s left open It should be cheep and funResearchFirst step was to search for existing solutions and I found really good sources: Make Your Garage Door Opener Smart How to create a Garage Door Opener in Home AssistantSo my project was based on these materials.I found out that: Home Assistant is the great platform for home automation and I decided to use it as a main controller The cheapest and easiest way to run home assistant is on Raspberry Pi. Plus I already have Raspberry Pi with 12 inch touch screen. I’ll use it for this project and for all following home automation endeavors Door opener switch could be by Sonoff or Shelly. I decided to go with Shelly 1 for 19$ There is wide range of door sensors, I thought to buy wireless (Zigbee) ones and stopped on Aqara Door and Window Sensor for 18$AssemblySwitchInstallation didn’t take a lot of time. It looks like:After Shelly 1 installation, a new WiFi network shelly-&lt;id&gt; is available that you can connect and configure the device by opening URL http://192.168.33.1I configured WiFi network on the device to use my home network and found new device in home assistantSensorsInstallation of these sensors where very easy: Open the box Press button for 3 seconds and wait for blinking blue light Open Home assistant and open ConBee II -&gt; devices Click on “Add Device” button Attach sensors to the garage doorHome AssistantSetupI setup home assistant on RaspberryPi according to instructions from https://www.home-assistant.io/installation/raspberrypi/In addition to the HA core I installed Red Node AddOn. I use it to create logic, flows in the home assistant. Mosquitto broker for MQTT protocol communicationMain dashboard looks like:AutomationsFirst I notice that when I enable the switch one time it disable the button in the garage that opens the door. It requires to toggle the Shelly switch one more time to enable the button on a wall.So, I decided to create a button on main dashboard that toggle the switch twice with 1/2 second delay. In order to create the button I added helper button:The flow in Node Red looks like: Event:state node “Button was pressed” is associated with the created button Call service node “Toggle garage door” Domain: switch Service: toggle Device: &lt; garage door switch &gt; Entity: &lt; switch.garage_opener &gt; Data: on Wait until node “Wait 1/2 second” wait 0.5 seconds and then Call service node “Second switch” is copy of node #2Second automation that I decided to create is inception of security system. I created Helper Toggle switch in HA Created flow that checks if the toggle is on and garage door got opened - i get the emailThe flow looks like evant:state node “Garage door got opened” monitors if sensor detects open door current state “Alarm system enabled” retrieves value of input_boolean.enable_security (the toggle that I created before) Switch “Is On” checks that value that we retrieved from previous step is on send email node send my an email…ConclusionNot bad solution for under 40$. Well, if we take into account also Raspberry Pi than whole project could cost bellow 200$. However, I can imagine realm of possibility in home automation field based on single Home Assistant platform.Also I decided to buy subscription https://www.home-assistant.io/cloud/ 65$ / year; it allows me to expose my home assistant to the internet. As a result I can use Home Assistant application on my phone anywhere:" }, { "title": "Blog with GitHub Pages and Jekyll", "url": "/posts/setup-personalized-blog-with-jekyll/", "categories": "config", "tags": "github, jekyll, workflow", "date": "2023-01-10 00:00:00 +0000", "snippet": "IntroductionThough it’s very controversial statement, I believe, we all should have a place in the ocean of the Internet. After some contemplation period I came to a conclusion that I need: Person...", "content": "IntroductionThough it’s very controversial statement, I believe, we all should have a place in the ocean of the Internet. After some contemplation period I came to a conclusion that I need: Personalized domain name Personalized email address Blog to share ideasAs of points 1 and 2, I registered the domain with GoDaddy and bought E-Mail service for 5$ a month. Also, I’ve created and registered a new user in the email.The issue was to connect the email server on my MacOS. However, it was an easy process by selecting System Preferences-&gt;Internet Accounts-&gt;Microsoft Echange. Login with your new user credentialsSo the only one problem with my idea was personalized blog. I had several requirements for the blog: Fully customizable Easy for editing Easy to add code snippets Source safeAs you can understand regular blogs by Google or by life journal didn’t work for me. Since I’m software developer I’m used to Markup language that is used in Confluence, Wiki and some other tools like Obsidian. As a result, I came to a conclusion that Markup blog with its content in source control like github is the best approach.Apparently GitHub already provides a service of hosting static pages - GitHub Pages.The following post is dedicated to instructions of setting up the blog. This post is actually a first post of the blog.Creating BlogThese instructions were created for MacOsInstall SoftwareInstall Homebrew/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"Install chruby and the latest Ruby with ruby-installInstall chruby and ruby-install with Homebrew:brew install chruby ruby-install xzInstall the latest stable version of Ruby (supported by Jekyll):ruby-install ruby 3.1.3This operation took some time… So be patientecho \"source $(brew --prefix)/opt/chruby/share/chruby/chruby.sh\" &gt;&gt; ~/.zshrcecho \"source $(brew --prefix)/opt/chruby/share/chruby/auto.sh\" &gt;&gt; ~/.zshrcecho \"chruby ruby-3.1.3\" &gt;&gt; ~/.zshrc # run 'chruby' to see actual versionInstall Jekyllgem install jekyll bundlerConfigure GithubCreate new repositoryGive a name to the repository like &lt;github-user-name&gt;.github.ioCreate 2 branches main gh-pagesBuild Blog with JekyllClone repositorygit clone https://github.com/&lt;user-name&gt;/&lt;user-name&gt;.github.io.gitcd &lt;user-name&gt;.github.ioJekyll setupjekyll new --skip-bundle .It will create default version of the blog. I did several changes and my project looks like:Theme selectionI didn’t like the minima theme provided out of the box, so I was looking for a prettier design. I decided to use theme jekyll-theme-tao. To do that:Change _config.yml# Build settingstheme: jekyll-theme-tao# _plugins:# - jekyll-remote-theme# - jekyll-feedChange Gemfile...gem \"jekyll-theme-tao\"...GitHub ActionGitHub provides an “actions” functionality, that allows to execute a build/task in a docker container. We are going to write a workflow that builds the site, merges the changes to gh-pages branch, and deploy the site to GitHub pages.But before that ensure that in your repository -&gt; Settings gh-pages branch is selected:Create file ./github/workflows/github-pages.yml with content:name: Build and deploy Jekyll site to GitHub Pageson: push: branches: - mainjobs: github-pages: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/cache@v3 with: path: vendor/bundle key: $-gems-$ restore-keys: | $-gems- - uses: jeffreytse/jekyll-deploy-action@v0.4.0 with: provider: 'github' token: $ # It's your Personal Access Token(PAT) repository: '' # Default is current repository branch: 'gh-pages' # Default is gh-pages for github provider jekyll_src: './' # Default is root directory jekyll_cfg: '_config.yml' # Default is _config.yml jekyll_baseurl: '' # Default is according to _config.yml bundler_ver: '&gt;=0' # Default is latest bundler version cname: '' # Default is to not use a cname actor: '' # Default is the GITHUB_ACTOR pre_build_commands: ''Commit directory to the main branchThe workflow will detect the changes, pick it up, merge changes to the gh-pages branch and will deploy the site.Now the blog is available by URL: https://&lt;user-name&gt;.github.ioConfigure blog with custom DNSCreate CNAME in GoDaddy Type: CNAME Name: blog Data: t-boris.github.io.Configure custom domain in GitHubNow each time I’ll add Markup file to _posts directory the site will be updated, and it’s available by custom DNS.Full code of the blog is available by URL https://github.com/t-boris/t-boris.github.io" } ]
